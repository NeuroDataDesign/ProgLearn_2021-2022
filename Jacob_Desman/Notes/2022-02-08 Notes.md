# 2022-02-08 Progress Notes
Created: 2022-02-08, 14:38

## Discussion
- The intutition is that the rate of convergence for KDG (KDF/KDN) is faster than than RF
	- We have shown via simulations
	- Now need the theory
- First step: formulate the problem
	- Can draw inspiration from how others formalize label noise

## Some Papers
These were some papers that I was looking through in case they might be helpful for drawing inspiration to address the theory problems

- On the Robustnes of Decision Tree Learning under Label Noise
	- https://arxiv.org/abs/1605.06296
	- Has discussions on the difficulty of proofs for assymetric label noise, but has work for symmetric label noise
	- Lemmas 7 and 8, and remark 10 might be good to keep in mind
	- Overall helpful for showing how the problem works out
	- Also, given the proofs, some of those figures might also be good to reproduce if relevant
		- [Probabilistic Random Forest A machine learning algorithm for noisy datasets](https://arxiv.org/abs/1811.05994) also has some possibly good to show figures
- Classification with Noisy Labels by Importance Reweighting
	- https://arxiv.org/abs/1411.7718
- Analysis of a Random Forests Model
	- https://www.jmlr.org/papers/volume13/biau12a/biau12a.pdf